{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFUfZug944eL"
      },
      "source": [
        "## DDA3020 Homework 2\n",
        "### Instructions:\n",
        "- Follow the notebook and complete the code cells marked as TODO\n",
        "- Ensure your code runs successfully until the end of the notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ui0w21QN44eM",
        "outputId": "ea283ce8-c976-4349-881f-4a1a6f2df56d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "# ========== data info ============ #\n",
            "train validation data: (1000, 100)\n",
            "train validation label: (1000,)\n",
            "test data: (400, 100)\n",
            "test label: (400,)\n",
            "# ================================= #\n"
          ]
        }
      ],
      "source": [
        "from os import path as osp\n",
        "import numpy as np\n",
        "\n",
        "# load data\n",
        "def load_data():\n",
        "\n",
        "    data_dir = './data'\n",
        "    train_val_data_path = osp.join(data_dir, 'train_validation_data.npy')\n",
        "    train_val_label_path = osp.join(data_dir, 'train_validation_label.npy')\n",
        "    test_data_path = osp.join(data_dir, 'test_data.npy')\n",
        "    test_label_path = osp.join(data_dir, 'test_label.npy')\n",
        "\n",
        "    train_val_data = np.load(train_val_data_path)\n",
        "    train_val_label = np.load(train_val_label_path)\n",
        "    test_data = np.load(test_data_path)\n",
        "    test_label = np.load(test_label_path)\n",
        "    return train_val_data, train_val_label, test_data, test_label\n",
        "\n",
        "\n",
        "train_validation_data, train_validation_label, test_data, test_label = load_data()\n",
        "\n",
        "print(f'# ========== data info ============ #')\n",
        "print(f'train validation data: {train_validation_data.shape}')\n",
        "print(f'train validation label: {train_validation_label.shape}')\n",
        "print(f'test data: {test_data.shape}')\n",
        "print(f'test label: {test_label.shape}')\n",
        "print(f'# ================================= #')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q6DbPw4UZeLH"
      },
      "source": [
        "## Implementing the “train validation split” function module by yourself.\n",
        "(Note: You cannot change the inputs (arguments) and outputs (return values) of this function to make sure that the following code part can run correctly.\n",
        "You should first read all the code snippets and understand them, and then try to complete this function.\n",
        "Hint: You need to split training and validation set for the two classes evenly that means contains\n",
        "800 samples (Class-0 / Class-1 = 1:1) for training set and 200 samples (Class-0 / Class-1 = 1:1) for validation set in each split.)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nYba4xPo44eN"
      },
      "outputs": [],
      "source": [
        "# data split for K-fold Cross-validation\n",
        "\n",
        "def train_validation_split(K, train_val_data, train_val_label):\n",
        "                          # K (int): Number of folds.\n",
        "                          # train_val_data (np.ndarray): Training and validation data.\n",
        "                          # train_val_label (np.ndarray): Training and validation labels.\n",
        "    # TODO: ==========================\n",
        "    #Implementing the “train validation split” function module by yourself. (Note: You cannot\n",
        "    # change the inputs (arguments) and outputs (return values) of this function to make sure that\n",
        "    # the following code part can run correctly. You should first read all the code snippets and\n",
        "    # understand them, and then try to complete this function. Hint: You need to split training\n",
        "    # and validation set for the two classes evenly that means contains 800 samples (Class-0 / Class1 = 1:1) for training set and 200 samples (Class-0 / Class-1 = 1:1) for validation set in each\n",
        "    # split.)\n",
        "    # Shuffle the data while keeping the class balance\n",
        "    from sklearn.utils import shuffle\n",
        "\n",
        "    data_class0 = train_val_data[train_val_label == 0]\n",
        "    data_class1 = train_val_data[train_val_label == 1]\n",
        "\n",
        "    label_class0 = train_val_label[train_val_label == 0]\n",
        "    label_class1 = train_val_label[train_val_label == 1]\n",
        "\n",
        "    data_class0, label_class0 = shuffle(data_class0, label_class0, random_state=42)\n",
        "    data_class1, label_class1 = shuffle(data_class1, label_class1, random_state=42)\n",
        "\n",
        "    # Number of samples per class\n",
        "    n_class0 = len(data_class0)\n",
        "    n_class1 = len(data_class1)\n",
        "\n",
        "    # Calculate fold sizes\n",
        "    fold_size_class0 = n_class0 // K\n",
        "    fold_size_class1 = n_class1 // K\n",
        "\n",
        "    train_datas = []\n",
        "    train_labels = []\n",
        "    val_datas = []\n",
        "    val_labels = []\n",
        "\n",
        "    for i in range(K):\n",
        "        # Define the start and end indices for validation set\n",
        "        start_class0 = i * fold_size_class0\n",
        "        end_class0 = start_class0 + fold_size_class0\n",
        "        start_class1 = i * fold_size_class1\n",
        "        end_class1 = start_class1 + fold_size_class1\n",
        "\n",
        "        # Validation data for this fold\n",
        "        val_data_fold = np.concatenate((data_class0[start_class0:end_class0],\n",
        "                                        data_class1[start_class1:end_class1]), axis=0)\n",
        "        val_label_fold = np.concatenate((label_class0[start_class0:end_class0],\n",
        "                                         label_class1[start_class1:end_class1]), axis=0)\n",
        "\n",
        "        # Training data is all data not in validation fold\n",
        "        train_data_fold = np.concatenate((data_class0[:start_class0], data_class0[end_class0:],\n",
        "                                          data_class1[:start_class1], data_class1[end_class1:]), axis=0)\n",
        "        train_label_fold = np.concatenate((label_class0[:start_class0], label_class0[end_class0:],\n",
        "                                           label_class1[:start_class1], label_class1[end_class1:]), axis=0)\n",
        "\n",
        "        # Shuffle training and validation data\n",
        "        train_data_fold, train_label_fold = shuffle(train_data_fold, train_label_fold, random_state=42)\n",
        "        val_data_fold, val_label_fold = shuffle(val_data_fold, val_label_fold, random_state=42)\n",
        "\n",
        "        train_datas.append(train_data_fold)\n",
        "        train_labels.append(train_label_fold)\n",
        "        val_datas.append(val_data_fold)\n",
        "        val_labels.append(val_label_fold)\n",
        "\n",
        "    return train_datas, train_labels, val_datas, val_labels\n",
        "          # train_datas (list of np.ndarray): List of training data for each fold.\n",
        "          #     train_labels (list of np.ndarray): List of training labels for each fold.\n",
        "          #     val_datas (list of np.ndarray): List of validation data for each fold.\n",
        "          #     val_labels (list of np.ndarray): List of validation labels for each fold."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "myh56gGBMsSM"
      },
      "outputs": [],
      "source": [
        "train_datas, train_labels, validation_datas, validation_labels = train_validation_split(5, train_validation_data, train_validation_label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lXGl-YSeM2Ee",
        "outputId": "db480322-5685-4f07-eb9d-16aabbb8352a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "800"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(train_labels[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "z-xH5cz9MxHd",
        "outputId": "09f93c06-d6f9-4a8f-a2a2-f6a2a7de2a7b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "400.0"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_labels[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r3PJNhA444eN"
      },
      "outputs": [],
      "source": [
        "# evaluation metrics\n",
        "\n",
        "def eva_precision(true_label, pred_label, _class):\n",
        "    # \"Of all the instances predicted as positive, how many were actually positive?\"\n",
        "    # TODO: ==========================\n",
        "    TP = np.sum((pred_label == _class) & (true_label == _class))\n",
        "    FP = np.sum((pred_label == _class) & (true_label != _class))\n",
        "    if TP + FP == 0:\n",
        "        return 0.0\n",
        "    precision = TP / (TP + FP)\n",
        "    return precision\n",
        "\n",
        "def eva_recall(true_label, pred_label, _class):\n",
        "    # \"Of all the actual positives, how many did the model correctly identify?\"\n",
        "    # TODO: ==========================\n",
        "    TP = np.sum((pred_label == _class) & (true_label == _class))\n",
        "    FN = np.sum((pred_label != _class) & (true_label == _class))\n",
        "    if TP + FN == 0:\n",
        "        return 0.0\n",
        "    recall = TP / (TP + FN)\n",
        "    return recall\n",
        "\n",
        "def eva_f1(true_label, pred_label, _class):\n",
        "    # F1 Score is the harmonic mean of Precision and Recall. It is useful when you need to balance Precision and Recall.\n",
        "    # A high F1 score means that both Precision and Recall are high. This is particularly useful in cases where the data classes are imbalanced.\n",
        "    # TODO: ==========================\n",
        "    precision = eva_precision(true_label, pred_label, _class)\n",
        "    recall = eva_recall(true_label, pred_label, _class)\n",
        "    if precision + recall == 0:\n",
        "        return 0.0\n",
        "    f1 = (2 * precision * recall) / (precision + recall)\n",
        "    return f1\n",
        "\n",
        "def eva_accuracy(true_label, pred_label):\n",
        "    # \"Of all the instances, how many did the model predict correctly?\"\n",
        "    # TODO: ==========================\n",
        "    correct = np.sum(true_label == pred_label)\n",
        "    total = len(true_label)\n",
        "    accuracy = correct / total\n",
        "    return accuracy\n",
        "\n",
        "def eva_auroc(true_label, pred_label):\n",
        "    #Area Under the ROC Curve.\n",
        "    # AUROC measures the ability of a model to discriminate between the positive and negative classes.\n",
        "    # It evaluates the model’s performance across all classification thresholds.\n",
        "    # AUROC = 0.5: This means the model is no better than random guessing.\n",
        "    # AUROC = 1: This means the model perfectly distinguishes between the positive and negative classes.\n",
        "    # Unlike accuracy, AUROC takes into account the performance at all classification thresholds (from 0 to 1), rather than just a fixed threshold (usually 0.5).\n",
        "    # This is important in imbalanced datasets where accuracy may be misleading.\n",
        "    # For example, if 95% of your dataset is negative, predicting every sample as negative would result in a high accuracy (95%), but the model would fail to detect any positive class instances.\n",
        "\n",
        "    # TODO: ==========================\n",
        "    true_label = np.array(true_label)\n",
        "    pred_score = np.array(pred_label)\n",
        "\n",
        "    desc_score_indices = np.argsort(-pred_score)\n",
        "    sorted_scores = pred_score[desc_score_indices]\n",
        "    sorted_labels = true_label[desc_score_indices]\n",
        "\n",
        "    # Calculate True Positive Rate (TPR) and False Positive Rate (FPR)\n",
        "    TPR = []\n",
        "    FPR = []\n",
        "    P = np.sum(true_label == 1)\n",
        "    N = np.sum(true_label == 0)\n",
        "    TP = 0\n",
        "    FP = 0\n",
        "    prev_score = -np.inf\n",
        "\n",
        "    for score, label in zip(sorted_scores, sorted_labels):\n",
        "        if score != prev_score:\n",
        "            TPR.append(TP / P if P != 0 else 0)\n",
        "            FPR.append(FP / N if N != 0 else 0)\n",
        "            prev_score = score\n",
        "        if label == 1:\n",
        "            TP += 1\n",
        "        else:\n",
        "            FP += 1\n",
        "    TPR.append(TP / P if P != 0 else 0)\n",
        "    FPR.append(FP / N if N != 0 else 0)\n",
        "\n",
        "    # Calculate AUROC using the trapezoidal rule\n",
        "    auroc = 0.0\n",
        "    for i in range(1, len(TPR)):\n",
        "        auroc += (FPR[i] - FPR[i-1]) * (TPR[i] + TPR[i-1]) / 2\n",
        "    return auroc\n",
        "\n",
        "def evaluation(true_label, pred_label, _class):\n",
        "\n",
        "    precision = eva_precision(true_label, pred_label, _class)\n",
        "    recall = eva_recall(true_label, pred_label, _class)\n",
        "    f1 = eva_f1(true_label, pred_label, _class)\n",
        "    accuracy = eva_accuracy(true_label, pred_label)\n",
        "    auroc = eva_auroc(true_label, pred_label)\n",
        "\n",
        "    return {'precision': precision, 'recall': recall, 'f1': f1, 'accuracy': accuracy, 'auroc': auroc}\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WwAHhoGR44eN",
        "outputId": "bb6b02ae-a1e1-4d42-ef57-6e961a142dae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# ======================= 1-th time validation ======================= #\n",
            "Algorithm: [logistic regression] =========================\n",
            "hyper-parameter: {'penalty': 'l1'}\n",
            "F1 (Val set of Class-0): 0.94\n",
            "F1 (Val set of Class-1): 0.94\n",
            "Average F1 Linear Regression: 0.94\n",
            "Algorithm: [SVM] =========================================\n",
            "hyper-parameter: {'C': 1e-05}\n",
            "F1 (Val set of Class-0): 0.93\n",
            "F1 (Val set of Class-1): 0.93\n",
            "Average F1 SVM: 0.93\n",
            "# ======================= 2-th time validation ======================= #\n",
            "Algorithm: [logistic regression] =========================\n",
            "hyper-parameter: {'penalty': 'l1'}\n",
            "F1 (Val set of Class-0): 0.92\n",
            "F1 (Val set of Class-1): 0.91\n",
            "Average F1 Linear Regression: 0.91\n",
            "Algorithm: [SVM] =========================================\n",
            "hyper-parameter: {'C': 1e-05}\n",
            "F1 (Val set of Class-0): 0.95\n",
            "F1 (Val set of Class-1): 0.95\n",
            "Average F1 SVM: 0.95\n",
            "# ======================= 3-th time validation ======================= #\n",
            "Algorithm: [logistic regression] =========================\n",
            "hyper-parameter: {'penalty': 'l1'}\n",
            "F1 (Val set of Class-0): 0.94\n",
            "F1 (Val set of Class-1): 0.94\n",
            "Average F1 Linear Regression: 0.94\n",
            "Algorithm: [SVM] =========================================\n",
            "hyper-parameter: {'C': 1e-05}\n",
            "F1 (Val set of Class-0): 0.96\n",
            "F1 (Val set of Class-1): 0.96\n",
            "Average F1 SVM: 0.96\n",
            "# ======================= 4-th time validation ======================= #\n",
            "Algorithm: [logistic regression] =========================\n",
            "hyper-parameter: {'penalty': 'l1'}\n",
            "F1 (Val set of Class-0): 0.96\n",
            "F1 (Val set of Class-1): 0.96\n",
            "Average F1 Linear Regression: 0.96\n",
            "Algorithm: [SVM] =========================================\n",
            "hyper-parameter: {'C': 1e-05}\n",
            "F1 (Val set of Class-0): 0.96\n",
            "F1 (Val set of Class-1): 0.97\n",
            "Average F1 SVM: 0.96\n",
            "# ======================= 5-th time validation ======================= #\n",
            "Algorithm: [logistic regression] =========================\n",
            "hyper-parameter: {'penalty': 'l1'}\n",
            "F1 (Val set of Class-0): 0.97\n",
            "F1 (Val set of Class-1): 0.97\n",
            "Average F1 Linear Regression: 0.97\n",
            "Algorithm: [SVM] =========================================\n",
            "hyper-parameter: {'C': 1e-05}\n",
            "F1 (Val set of Class-0): 0.98\n",
            "F1 (Val set of Class-1): 0.98\n",
            "Average F1 SVM: 0.98\n"
          ]
        }
      ],
      "source": [
        "# model training and hyper-parameters fine-tuning\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "K = 5\n",
        "\n",
        "# hyper-parameter for logistic regression\n",
        "hyper_parameters_logistic_regression = {\n",
        "\n",
        "    # TODO: please choose different values to tune the model\n",
        "    'penalty': 'l1', # ['l1', 'l2']\n",
        "    #penalty='l1' applies Lasso Regularization (for sparse models).\n",
        "    #penalty='l2' applies Ridge Regularization (for smoother models).\n",
        "}\n",
        "\n",
        "# hyper-parameter for SVM\n",
        "hyper_parameters_svm = {\n",
        "\n",
        "    # TODO: please choose different values to tune the model\n",
        "    'C': 1e-5, # [1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1]\n",
        "    #Small C values impose more regularization (higher bias, lower variance).\n",
        "    #Large C values reduce regularization, focusing more on fitting the training data (lower bias, higher variance).\n",
        "}\n",
        "\n",
        "# obtain cross-validation set\n",
        "train_datas, train_labels, validation_datas, validation_labels = train_validation_split(K, train_validation_data, train_validation_label)\n",
        "\n",
        "\n",
        "for i, (train_data, train_label, validation_data, validation_label) in enumerate(zip(train_datas, train_labels, validation_datas, validation_labels)):\n",
        "\n",
        "    print(f'# ======================= {i + 1}-th time validation ======================= #')\n",
        "\n",
        "    # logistic regression\n",
        "\n",
        "    print(f'Algorithm: [logistic regression] =========================')\n",
        "    print(f'hyper-parameter: {hyper_parameters_logistic_regression}')\n",
        "    lr_model = LogisticRegression(solver='liblinear', **hyper_parameters_logistic_regression).fit(train_data, train_label)\n",
        "\n",
        "    # performance evaluation on validation set for tuning hyper-parameters\n",
        "    pred_label1 = lr_model.predict(validation_data)\n",
        "    F1_LR_0 = eva_f1(validation_label, pred_label1, _class=0)\n",
        "    print(f'F1 (Val set of Class-0): {F1_LR_0:.2f}')\n",
        "    F1_LR_1 = eva_f1(validation_label, pred_label1, _class=1)\n",
        "    print(f'F1 (Val set of Class-1): {F1_LR_1:.2f}')\n",
        "    F1_LR_average = (F1_LR_0 + F1_LR_1) / 2\n",
        "    print(f'Average F1 Linear Regression: {F1_LR_average:.2f}')\n",
        "\n",
        "    # SVM\n",
        "\n",
        "    print(f'Algorithm: [SVM] =========================================')\n",
        "    print(f'hyper-parameter: {hyper_parameters_svm}')\n",
        "    svm_model = SVC(kernel='linear', **hyper_parameters_svm).fit(train_data, train_label)\n",
        "\n",
        "    # performance evaluation on validation set for tuning hyper-parameters\n",
        "    pred_label2 = svm_model.predict(validation_data)\n",
        "    F1_SVM_0 = eva_f1(validation_label, pred_label2, _class=0)\n",
        "    print(f'F1 (Val set of Class-0): {F1_SVM_0:.2f}')\n",
        "    F1_SVM_1 = eva_f1(validation_label, pred_label2, _class=1)\n",
        "    print(f'F1 (Val set of Class-1): {F1_SVM_1:.2f}')\n",
        "    F1_SVM_average = (F1_SVM_0 + F1_SVM_1) / 2\n",
        "    print(f'Average F1 SVM: {F1_SVM_average:.2f}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hko9pQOq44eN",
        "outputId": "ca8f4ec9-bf6c-4ca3-9852-a6638edbc250"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# ======================= 1-th time validation ======================= #\n",
            "Algorithm: [logistic regression] =========================\n",
            "hyper-parameter: l1\n",
            "Result Class 0 (Test set): {'precision': 0.9292929292929293, 'recall': 0.92, 'f1': 0.9246231155778895, 'accuracy': 0.925, 'auroc': 0.9250000000000002}\n",
            "Result Class 1 (Test set): {'precision': 0.9207920792079208, 'recall': 0.93, 'f1': 0.9253731343283582, 'accuracy': 0.925, 'auroc': 0.9250000000000002}\n",
            "Algorithm: [SVM] =========================================\n",
            "hyper-parameter: 1e-05\n",
            "Result Class 0 (Test set): {'precision': 0.9441624365482234, 'recall': 0.93, 'f1': 0.9370277078085644, 'accuracy': 0.9375, 'auroc': 0.9374999999999998}\n",
            "Result Class 1 (Test set): {'precision': 0.9310344827586207, 'recall': 0.945, 'f1': 0.9379652605459057, 'accuracy': 0.9375, 'auroc': 0.9374999999999998}\n",
            "# ======================= 2-th time validation ======================= #\n",
            "Algorithm: [logistic regression] =========================\n",
            "hyper-parameter: l1\n",
            "Result Class 0 (Test set): {'precision': 0.8926829268292683, 'recall': 0.915, 'f1': 0.9037037037037038, 'accuracy': 0.9025, 'auroc': 0.9025000000000001}\n",
            "Result Class 1 (Test set): {'precision': 0.9128205128205128, 'recall': 0.89, 'f1': 0.9012658227848102, 'accuracy': 0.9025, 'auroc': 0.9025000000000001}\n",
            "Algorithm: [SVM] =========================================\n",
            "hyper-parameter: 1e-05\n",
            "Result Class 0 (Test set): {'precision': 0.9432989690721649, 'recall': 0.915, 'f1': 0.9289340101522843, 'accuracy': 0.93, 'auroc': 0.9299999999999999}\n",
            "Result Class 1 (Test set): {'precision': 0.9174757281553398, 'recall': 0.945, 'f1': 0.9310344827586207, 'accuracy': 0.93, 'auroc': 0.9299999999999999}\n",
            "# ======================= 3-th time validation ======================= #\n",
            "Algorithm: [logistic regression] =========================\n",
            "hyper-parameter: l1\n",
            "Result Class 0 (Test set): {'precision': 0.916256157635468, 'recall': 0.93, 'f1': 0.923076923076923, 'accuracy': 0.9225, 'auroc': 0.9224999999999999}\n",
            "Result Class 1 (Test set): {'precision': 0.9289340101522843, 'recall': 0.915, 'f1': 0.9219143576826196, 'accuracy': 0.9225, 'auroc': 0.9224999999999999}\n",
            "Algorithm: [SVM] =========================================\n",
            "hyper-parameter: 0.001\n",
            "Result Class 0 (Test set): {'precision': 0.9336734693877551, 'recall': 0.915, 'f1': 0.9242424242424242, 'accuracy': 0.925, 'auroc': 0.925}\n",
            "Result Class 1 (Test set): {'precision': 0.9166666666666666, 'recall': 0.935, 'f1': 0.9257425742574258, 'accuracy': 0.925, 'auroc': 0.925}\n",
            "# ======================= 4-th time validation ======================= #\n",
            "Algorithm: [logistic regression] =========================\n",
            "hyper-parameter: l1\n",
            "Result Class 0 (Test set): {'precision': 0.9387755102040817, 'recall': 0.92, 'f1': 0.9292929292929293, 'accuracy': 0.93, 'auroc': 0.9299999999999999}\n",
            "Result Class 1 (Test set): {'precision': 0.9215686274509803, 'recall': 0.94, 'f1': 0.9306930693069307, 'accuracy': 0.93, 'auroc': 0.9299999999999999}\n",
            "Algorithm: [SVM] =========================================\n",
            "hyper-parameter: 1e-05\n",
            "Result Class 0 (Test set): {'precision': 0.9581151832460733, 'recall': 0.915, 'f1': 0.9360613810741688, 'accuracy': 0.9375, 'auroc': 0.9375}\n",
            "Result Class 1 (Test set): {'precision': 0.9186602870813397, 'recall': 0.96, 'f1': 0.9388753056234719, 'accuracy': 0.9375, 'auroc': 0.9375}\n",
            "# ======================= 5-th time validation ======================= #\n",
            "Algorithm: [logistic regression] =========================\n",
            "hyper-parameter: l1\n",
            "Result Class 0 (Test set): {'precision': 0.9253731343283582, 'recall': 0.93, 'f1': 0.9276807980049875, 'accuracy': 0.9275, 'auroc': 0.9275}\n",
            "Result Class 1 (Test set): {'precision': 0.9296482412060302, 'recall': 0.925, 'f1': 0.9273182957393483, 'accuracy': 0.9275, 'auroc': 0.9275}\n",
            "Algorithm: [SVM] =========================================\n",
            "hyper-parameter: 1e-05\n",
            "Result Class 0 (Test set): {'precision': 0.9639175257731959, 'recall': 0.935, 'f1': 0.949238578680203, 'accuracy': 0.95, 'auroc': 0.95}\n",
            "Result Class 1 (Test set): {'precision': 0.9368932038834952, 'recall': 0.965, 'f1': 0.9507389162561576, 'accuracy': 0.95, 'auroc': 0.95}\n"
          ]
        }
      ],
      "source": [
        "# performance evaluation on test set\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "K = 5\n",
        "\n",
        "# hyper-parameter penlty for logistic regression. Hint: len(penalty) = 5\n",
        "penalty = [\n",
        "\n",
        "    # TODO: the optimal parameter selection for each split\n",
        "    'l1', 'l1', 'l1', 'l1', 'l1'\n",
        "]\n",
        "\n",
        "\n",
        "# hyper-parameter C for SVM. Hint: len(C) = 5\n",
        "C = [\n",
        "    # TODO: the optimal parameter selection for each split\n",
        "    1e-5, 1e-5, 1e-3, 1e-5, 1e-5\n",
        "]\n",
        "\n",
        "\n",
        "# obtain training data\n",
        "train_datas, train_labels, _, _ = train_validation_split(K, train_validation_data, train_validation_label)\n",
        "\n",
        "\n",
        "for i, (train_data, train_label) in enumerate(zip(train_datas, train_labels)):\n",
        "\n",
        "    print(f'# ======================= {i + 1}-th time validation ======================= #')\n",
        "\n",
        "    # logistic regression\n",
        "\n",
        "    print(f'Algorithm: [logistic regression] =========================')\n",
        "    print(f'hyper-parameter: {penalty[i]}')\n",
        "    lr_model = LogisticRegression(solver='liblinear', penalty=penalty[i]).fit(train_data, train_label)\n",
        "\n",
        "\n",
        "    # performance evaluation on test set\n",
        "    pred_label = lr_model.predict(test_data)\n",
        "    results_0 = evaluation(test_label, pred_label, _class=0)\n",
        "    results_1 = evaluation(test_label, pred_label, _class=1)\n",
        "    print(f'Result Class 0 (Test set): {results_0}')\n",
        "    print(f'Result Class 1 (Test set): {results_1}')\n",
        "\n",
        "    # SVM\n",
        "\n",
        "    print(f'Algorithm: [SVM] =========================================')\n",
        "    print(f'hyper-parameter: {C[i]}')\n",
        "    svm_model = SVC(kernel='linear', C=C[i]).fit(train_data, train_label)\n",
        "\n",
        "    # performance evaluation on test set\n",
        "    pred_label = svm_model.predict(test_data)\n",
        "    results_0 = evaluation(test_label, pred_label, _class=0)\n",
        "    results_1 = evaluation(test_label, pred_label, _class=1)\n",
        "    print(f'Result Class 0 (Test set): {results_0}')\n",
        "    print(f'Result Class 1 (Test set): {results_1}')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}